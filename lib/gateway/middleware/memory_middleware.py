"""
CCB Gateway Memory Middleware
åœ¨ Gateway å±‚è‡ªåŠ¨å¤„ç†è®°å¿†çš„è®°å½•å’Œæ³¨å…¥

v2.0 Enhancement: Heuristic Retrieval with Î±R + Î²I + Î³T scoring
"""

import asyncio
import json
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from lib.memory.memory_v2 import CCBLightMemory
from lib.memory.registry import CCBRegistry
from lib.skills.skills_discovery import SkillsDiscoveryService
from .system_context import SystemContextBuilder

# v2.0: Heuristic Retriever
try:
    from lib.memory.heuristic_retriever import HeuristicRetriever, ScoredMemory
    HAS_HEURISTIC = True
except ImportError:
    HAS_HEURISTIC = False
    print("[MemoryMiddleware] Warning: HeuristicRetriever not available, using basic search")


class MemoryMiddleware:
    """Gateway è®°å¿†ä¸­é—´ä»¶ (v2.0: Heuristic Retrieval)"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.memory = CCBLightMemory()
        self.registry = CCBRegistry()

        # åŠ è½½é…ç½®
        self.config = config or self._load_config()
        self.enabled = self.config.get("memory", {}).get("enabled", True)
        self.auto_inject = self.config.get("memory", {}).get("auto_inject", True)
        self.auto_record = self.config.get("memory", {}).get("auto_record", True)
        self.max_injected = self.config.get("memory", {}).get("max_injected_memories", 5)
        self.inject_system_context = self.config.get("memory", {}).get("inject_system_context", True)

        # é¢„åŠ è½½ç³»ç»Ÿä¸Šä¸‹æ–‡ï¼ˆSkillsã€MCPã€Providersï¼‰
        self.system_context = SystemContextBuilder()

        # ğŸ†• åˆå§‹åŒ– Skills Discovery Service
        self.skills_discovery = SkillsDiscoveryService()
        self.enable_skill_discovery = self.config.get("skills", {}).get("auto_discover", True)

        # ğŸ†• v2.0: å¯å‘å¼æ£€ç´¢å™¨
        self.heuristic_retriever = None
        self.use_heuristic = self.config.get("memory", {}).get("use_heuristic_retrieval", True)
        if HAS_HEURISTIC and self.use_heuristic:
            try:
                self.heuristic_retriever = HeuristicRetriever()
                print(f"[MemoryMiddleware] Heuristic retriever initialized")
            except Exception as e:
                print(f"[MemoryMiddleware] Heuristic retriever init error: {e}")

        print(f"[MemoryMiddleware] Initialized (enabled={self.enabled}, heuristic={self.heuristic_retriever is not None})")
        print(f"[MemoryMiddleware] System context preloaded: {self.system_context.get_stats()}")
        print(f"[MemoryMiddleware] Skills discovery: {self.enable_skill_discovery}")

    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_file = Path.home() / ".ccb" / "gateway_config.json"

        if config_file.exists():
            with open(config_file) as f:
                return json.load(f)

        # é»˜è®¤é…ç½®
        return {
            "memory": {
                "enabled": True,
                "auto_inject": True,
                "auto_record": True,
                "max_injected_memories": 5,
                "inject_system_context": True,  # æ–°å¢ï¼šæ³¨å…¥ç³»ç»Ÿä¸Šä¸‹æ–‡
                "injection_strategy": "recent_plus_relevant",
                "use_heuristic_retrieval": True  # v2.0: ä½¿ç”¨å¯å‘å¼æ£€ç´¢
            },
            "skills": {
                "auto_discover": True,  # ğŸ†• è‡ªåŠ¨å‘ç°ç›¸å…³æŠ€èƒ½
                "recommend_skills": True,  # ğŸ†• æ¨èæŠ€èƒ½ç»™ç”¨æˆ·
                "max_recommendations": 3  # ğŸ†• æœ€å¤šæ¨èæŠ€èƒ½æ•°
            },
            "recommendation": {
                "enabled": True,
                "auto_switch_provider": False,
                "confidence_threshold": 0.7
            }
        }

    async def pre_request(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¯·æ±‚å‰å¤„ç†ï¼ˆPre-Request Hookï¼‰

        åŠŸèƒ½ï¼š
        1. æå–ä»»åŠ¡å…³é”®è¯
        2. æœç´¢ç›¸å…³è®°å¿†
        3. æ¨èæœ€ä½³ Provider
        4. æ³¨å…¥ä¸Šä¸‹æ–‡åˆ° prompt
        """
        if not self.enabled or not self.auto_inject:
            return request

        provider = request.get("provider")
        message = request.get("message", "")
        user_id = request.get("user_id", "default")

        print(f"[MemoryMiddleware] Pre-request: provider={provider}, message_len={len(message)}")

        # 1. æå–ä»»åŠ¡å…³é”®è¯
        keywords = self._extract_keywords(message)
        print(f"[MemoryMiddleware] Extracted keywords: {keywords}")

        # ğŸ†• 1.5. Skills Discovery - å‘ç°ç›¸å…³æŠ€èƒ½
        skill_recommendations = None
        if self.enable_skill_discovery:
            try:
                skill_recommendations = self.skills_discovery.get_recommendations(message)
                if skill_recommendations['found']:
                    print(f"[MemoryMiddleware] {skill_recommendations['message']}")
            except Exception as e:
                print(f"[MemoryMiddleware] Skills discovery error: {e}")

        # 2. æœç´¢ç›¸å…³è®°å¿† (v2.0: ä½¿ç”¨å¯å‘å¼æ£€ç´¢)
        relevant_memories = []
        heuristic_results = []  # v2.0: ä¿å­˜è¯„åˆ†ç»“æœ
        if keywords:
            try:
                if self.heuristic_retriever:
                    # v2.0: ä½¿ç”¨ HeuristicRetriever çš„ Î±R + Î²I + Î³T è¯„åˆ†
                    heuristic_results = self.heuristic_retriever.retrieve(
                        " ".join(keywords),
                        limit=self.max_injected,
                        request_id=request.get("request_id"),
                        track_access=True
                    )
                    # è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
                    relevant_memories = [
                        {
                            "id": m.memory_id,
                            "message_id": m.memory_id,
                            "provider": m.provider,
                            "question": "",
                            "answer": m.content[:300] if m.role == 'assistant' else m.content[:300],
                            "timestamp": m.timestamp,
                            "relevance_score": m.relevance_score,
                            "importance_score": m.importance_score,
                            "recency_score": m.recency_score,
                            "final_score": m.final_score
                        }
                        for m in heuristic_results
                    ]
                    print(f"[MemoryMiddleware] Heuristic search: found {len(relevant_memories)} memories")
                else:
                    # å›é€€åˆ°åŸºæœ¬æœç´¢
                    relevant_memories = self.memory.search_conversations(
                        " ".join(keywords),
                        limit=self.max_injected
                    )
                    print(f"[MemoryMiddleware] Basic search: found {len(relevant_memories)} memories")
            except Exception as e:
                print(f"[MemoryMiddleware] Search error: {e}")

        # 3. æ¨èæœ€ä½³ Providerï¼ˆå¦‚æœå¯ç”¨ï¼‰
        print(f"[MemoryMiddleware] Provider before recommendation: {provider}")
        recommendation_config = self.config.get("recommendation", {})
        if recommendation_config.get("enabled", True) and provider in ["auto", None]:
            print(f"[MemoryMiddleware] Entering recommendation logic (provider={provider})")
            try:
                recommendations = self.registry.recommend_provider(keywords)
                if recommendations:
                    recommended_provider = recommendations[0]["provider"]
                    reason = recommendations[0]["reason"]

                    print(f"[MemoryMiddleware] Recommended: {recommended_provider} ({reason})")

                    if recommendation_config.get("auto_switch_provider", False):
                        print(f"[MemoryMiddleware] Auto-switching provider: {provider} -> {recommended_provider}")
                        request["provider"] = recommended_provider
                        request["_recommendation"] = {
                            "provider": recommended_provider,
                            "reason": reason,
                            "auto_switched": True
                        }
            except Exception as e:
                print(f"[MemoryMiddleware] Recommendation error: {e}")

        # 4. æ³¨å…¥ä¸Šä¸‹æ–‡ï¼ˆåŒ…æ‹¬ç³»ç»Ÿä¸Šä¸‹æ–‡å’Œç›¸å…³è®°å¿†ï¼‰
        try:
            context_parts = []

            # 4a. æ³¨å…¥é¢„åŸ‹çš„ç³»ç»Ÿä¸Šä¸‹æ–‡ï¼ˆSkillsã€MCPã€Providersï¼‰
            if self.inject_system_context:
                system_ctx = self.system_context.get_relevant_context(
                    keywords,
                    provider or request.get("provider", "unknown")
                )
                if system_ctx:
                    context_parts.append(system_ctx)
                    print(f"[MemoryMiddleware] System context injected")

            # 4b. æ³¨å…¥ç›¸å…³è®°å¿†
            if relevant_memories:
                memory_ctx = self._format_memory_context(relevant_memories)
                if memory_ctx:
                    context_parts.append(memory_ctx)
                    print(f"[MemoryMiddleware] {len(relevant_memories)} memories injected")

            # ğŸ†• 4c. æ³¨å…¥æŠ€èƒ½æ¨èï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰
            if skill_recommendations and skill_recommendations['found']:
                skills_ctx = self._format_skills_context(skill_recommendations)
                if skills_ctx:
                    context_parts.append(skills_ctx)
                    print(f"[MemoryMiddleware] Skills recommendations injected")

            # åˆå¹¶ä¸Šä¸‹æ–‡
            if context_parts:
                full_context = "\n\n".join(context_parts)

                # å¢å¼ºåŸå§‹æ¶ˆæ¯
                request["message"] = f"""# ç³»ç»Ÿä¸Šä¸‹æ–‡

{full_context}

---

# ç”¨æˆ·è¯·æ±‚
{message}
"""
                request["_memory_injected"] = True
                request["_memory_count"] = len(relevant_memories)
                request["_system_context_injected"] = self.inject_system_context
                request["_skills_recommended"] = bool(skill_recommendations and skill_recommendations['found'])

                # ğŸ†• Phase 1: è¿½è¸ªæ³¨å…¥è¯¦æƒ…ï¼ˆå¦‚æœæœ‰ request_idï¼‰
                request_id = request.get("request_id")
                if request_id:
                    self._track_injection(
                        request_id=request_id,
                        provider=provider,
                        original_message=message,
                        memories=relevant_memories,
                        skills=skill_recommendations,
                        system_context_injected=self.inject_system_context
                    )

        except Exception as e:
            print(f"[MemoryMiddleware] Context injection error: {e}")

        return request

    async def post_response(self, request: Dict[str, Any], response: Dict[str, Any]):
        """
        å“åº”åå¤„ç†ï¼ˆPost-Response Hookï¼‰

        åŠŸèƒ½ï¼š
        1. è®°å½•å¯¹è¯
        2. æ›´æ–°ç»Ÿè®¡
        3. ï¼ˆå¯é€‰ï¼‰æå–å…³é”®äº‹å®
        """
        if not self.enabled or not self.auto_record:
            return

        try:
            provider = request.get("provider", "unknown")
            message = request.get("message", "")

            # ç§»é™¤æ³¨å…¥çš„ä¸Šä¸‹æ–‡ï¼Œåªä¿å­˜åŸå§‹é—®é¢˜
            if request.get("_memory_injected"):
                # æå–åŸå§‹é—®é¢˜ï¼ˆåœ¨ "# ç”¨æˆ·è¯·æ±‚" ä¹‹åï¼‰
                parts = message.split("# ç”¨æˆ·è¯·æ±‚")
                if len(parts) > 1:
                    message = parts[1].strip()

            response_text = response.get("response", "")

            metadata = {
                "model": request.get("model"),
                "latency_ms": response.get("latency_ms"),
                "tokens": response.get("tokens"),
                "memory_injected": request.get("_memory_injected", False),
                "memory_count": request.get("_memory_count", 0)
            }

            # è®°å½•å¯¹è¯
            self.memory.record_conversation(
                provider=provider,
                question=message,
                answer=response_text,
                metadata=metadata
            )

            print(f"[MemoryMiddleware] Conversation recorded: provider={provider}")

            # ğŸ†• è®°å½•æŠ€èƒ½ä½¿ç”¨ï¼ˆå¦‚æœå“åº”ä¸­æåˆ°äº†æŠ€èƒ½ï¼‰
            if self.enable_skill_discovery:
                self._record_skill_usage(request, response)

            # æ›´æ–°ç»Ÿè®¡ï¼ˆç”¨äºæ¨èä¼˜åŒ–ï¼‰
            # self.registry.update_usage_stats(provider, metadata)

        except Exception as e:
            print(f"[MemoryMiddleware] Post-response error: {e}")

    def _extract_keywords(self, text: str) -> List[str]:
        """æå–ä»»åŠ¡å…³é”®è¯ï¼ˆv3: ä½¿ç”¨æœ¬åœ° LLM æå–è¯­ä¹‰å…³é”®è¯ï¼‰"""
        # å°è¯•ä½¿ç”¨ LLM æå–ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°æ­£åˆ™æå–
        try:
            return self._extract_keywords_with_llm(text)
        except Exception as e:
            print(f"[MemoryMiddleware] LLM extraction failed: {e}, fallback to regex")
            return self._extract_keywords_regex(text)

    def _extract_keywords_with_llm(self, text: str) -> List[str]:
        """
        ä½¿ç”¨ Ollama æ™ºèƒ½è·¯ç”±æå–å…³é”®è¯

        è·¯ç”±ç­–ç•¥ï¼š
        1. é¦–é€‰æœ¬åœ° qwen2.5:7bï¼ˆå¿«é€Ÿï¼Œæ— ç½‘ç»œä¾èµ–ï¼‰
        2. æœ¬åœ°è¶…æ—¶/å¤±è´¥ â†’ è‡ªåŠ¨åˆ‡æ¢äº‘ç«¯ deepseek-v3.1:671b-cloud
        3. äº‘ç«¯å¤±è´¥ â†’ å›é€€åˆ°æ­£åˆ™æå–
        """
        import requests
        import re

        # æ¸…ç†æ–‡æœ¬
        cleaned = re.sub(r'\s+', ' ', text).strip()

        # çŸ­æŸ¥è¯¢ç›´æ¥è¿”å›
        if len(cleaned) <= 10:
            return [cleaned]

        # æ„é€ æç¤ºè¯
        prompt = f"""ä»ä¸‹é¢çš„é—®é¢˜ä¸­æå–2-3ä¸ªæœ€æ ¸å¿ƒçš„å…³é”®è¯ï¼ˆåè¯æˆ–åè¯çŸ­è¯­ï¼‰ï¼Œç”¨é€—å·åˆ†éš”ã€‚
åªè¿”å›å…³é”®è¯ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚

é—®é¢˜ï¼š{cleaned}

å…³é”®è¯ï¼š"""

        # æ¨¡å‹è·¯ç”±é…ç½®
        models = [
            {
                'name': 'qwen2.5:7b',
                'timeout': 6,      # æœ¬åœ°æ¨¡å‹ 6 ç§’è¶…æ—¶ï¼ˆå†·å¯åŠ¨ ~5sï¼Œçƒ­è°ƒç”¨ <1sï¼‰
                'location': 'local'
            },
            {
                'name': 'deepseek-v3.1:671b-cloud',
                'timeout': 10,     # äº‘ç«¯æ¨¡å‹ 10 ç§’è¶…æ—¶
                'location': 'cloud'
            }
        ]

        last_error = None
        for model_config in models:
            model_name = model_config['name']
            timeout = model_config['timeout']
            location = model_config['location']

            try:
                response = requests.post(
                    'http://localhost:11434/api/generate',
                    json={
                        'model': model_name,
                        'prompt': prompt,
                        'stream': False,
                        'options': {
                            'temperature': 0.3,
                            'num_predict': 50
                        }
                    },
                    timeout=timeout
                )

                if response.status_code == 200:
                    result = response.json()
                    keywords_str = result.get('response', '').strip()

                    # è§£æå…³é”®è¯
                    keywords = []
                    raw_keywords = re.split(r'[,ï¼Œã€]', keywords_str)

                    for kw in raw_keywords:
                        cleaned_kw = re.sub(r'^[\d\.\sã€]+', '', kw.strip())
                        cleaned_kw = re.sub(r'[ã€‚ï¼ï¼Ÿ,.!?ã€]+$', '', cleaned_kw)
                        if cleaned_kw and len(cleaned_kw) >= 2:
                            keywords.append(cleaned_kw)

                    if keywords:
                        print(f"[MemoryMiddleware] LLM extracted ({location}:{model_name}): {keywords}")
                        return keywords[:5]

            except requests.exceptions.Timeout:
                print(f"[MemoryMiddleware] Ollama timeout ({timeout}s) for {location}:{model_name}")
                last_error = f"timeout:{model_name}"
                continue  # å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
            except requests.exceptions.ConnectionError:
                print(f"[MemoryMiddleware] Ollama not running on localhost:11434")
                last_error = "connection_error"
                break  # Ollama æœåŠ¡æœªè¿è¡Œï¼Œç›´æ¥é€€å‡º
            except Exception as e:
                print(f"[MemoryMiddleware] Ollama API error ({model_name}): {e}")
                last_error = str(e)
                continue  # å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹

        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥
        raise Exception(f"LLM extraction failed, fallback to regex")

    def _extract_keywords_regex(self, text: str) -> List[str]:
        """æ­£åˆ™æå–å…³é”®è¯ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        import re

        # æ¸…ç†ï¼šç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
        cleaned = re.sub(r'\s+', ' ', text).strip()

        # ä¸­æ–‡åœç”¨è¯ï¼ˆç–‘é—®è¯å’ŒåŠ©è¯ï¼‰
        stop_words = {
            "çš„", "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "äº†", "æˆ‘", "ä½ ", "ä»–", "å¥¹",
            "è¿™", "é‚£", "ä¸€ä¸ª", "æ€ä¹ˆ", "å¦‚ä½•", "ä»€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "éœ€è¦",
            "å¯ä»¥", "è¿˜", "åˆšæ‰", "æåˆ°", "è€ƒè™‘", "å—", "å‘¢", "å§", "è¦",
            "ä¼š", "èƒ½", "å°†", "è¢«", "æŠŠ", "å¯¹", "ç»™", "è®©", "å‘", "ä»",
            "æ³¨æ„", "å…³æ³¨", "æ€è€ƒ", "æƒ³è¦", "çŸ¥é“", "äº†è§£", "å“ªäº›",
        }

        # æå– 3-4 å­—çš„ä¸­æ–‡åè¯ï¼ˆé€šå¸¸æ˜¯å®ä½“è¯ï¼‰
        # å¦‚ï¼š"è´­ç‰©è½¦"ã€"ç”µå•†ç½‘ç«™"ã€"Reactç»„ä»¶"
        chinese_keywords = re.findall(r'[\u4e00-\u9fff]{3,4}', cleaned)

        # æå–è‹±æ–‡å•è¯ï¼ˆ3å­—æ¯ä»¥ä¸Šï¼‰
        english_keywords = re.findall(r'\b[a-zA-Z]{3,}\b', cleaned.lower())

        # è¿‡æ»¤åœç”¨è¯
        keywords = []
        for word in chinese_keywords + english_keywords:
            if word not in stop_words and len(word) >= 2:
                keywords.append(word)

        # å»é‡
        seen = set()
        unique_keywords = []
        for k in keywords:
            if k not in seen:
                seen.add(k)
                unique_keywords.append(k)

        # å¦‚æœæå–åˆ°å…³é”®è¯ï¼Œè¿”å›å‰5ä¸ªæœ€é‡è¦çš„
        # å¦‚æœæ²¡æœ‰å…³é”®è¯ï¼Œè¿”å›æ¸…ç†åçš„åŸæ–‡ï¼ˆçŸ­æŸ¥è¯¢ï¼‰
        if unique_keywords:
            return unique_keywords[:5]
        else:
            # å¯¹äºçŸ­æŸ¥è¯¢ï¼ˆå¦‚"è´­ç‰©è½¦"ï¼‰ï¼Œç›´æ¥è¿”å›
            return [cleaned] if len(cleaned) <= 10 else []

    def _format_memory_context(self, memories: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆv2.0: åŒ…å«è¯„åˆ†ä¿¡æ¯ï¼‰"""
        if not memories:
            return ""

        context_parts = ["## ğŸ’­ ç›¸å…³è®°å¿†"]

        for i, mem in enumerate(memories, 1):
            provider_name = mem.get("provider", "unknown")
            question = mem.get("question", "")[:100]
            answer = mem.get("answer", "")[:200]

            # v2.0: å¦‚æœæœ‰è¯„åˆ†ï¼Œæ˜¾ç¤ºè¯„åˆ†ä¿¡æ¯
            score_info = ""
            if mem.get("final_score") is not None:
                score_info = f" (score: {mem['final_score']:.2f})"

            context_parts.append(f"{i}. [{provider_name}]{score_info} {question}")
            context_parts.append(f"   A: {answer}...")
            context_parts.append("")

        return "\n".join(context_parts)

    def _format_skills_context(self, recommendations: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æŠ€èƒ½æ¨èä¸Šä¸‹æ–‡ï¼ˆğŸ†• æ–°å¢ï¼‰"""
        if not recommendations or not recommendations.get('found'):
            return ""

        context_parts = ["## ğŸ› ï¸ ç›¸å…³æŠ€èƒ½æ¨è"]

        for skill in recommendations.get('skills', []):
            name = skill['name']
            description = skill['description']
            installed = skill['installed']
            relevance = skill['relevance_score']

            if installed:
                # å·²å®‰è£…çš„æŠ€èƒ½
                context_parts.append(
                    f"- **/{name}** (score: {relevance}) - {description}"
                )
                context_parts.append(f"  âœ“ å·²å®‰è£…ï¼Œå¯ç›´æ¥ä½¿ç”¨: `/{name}`")
            else:
                # æœªå®‰è£…çš„æŠ€èƒ½
                context_parts.append(
                    f"- **{name}** (score: {relevance}) - {description}"
                )
                context_parts.append(f"  âš ï¸ æœªå®‰è£…ï¼Œå»ºè®®å®‰è£…åä½¿ç”¨")

        return "\n".join(context_parts)

    def _format_context(
        self,
        memories: List[Dict[str, Any]],
        keywords: List[str],
        provider: str
    ) -> str:
        """æ ¼å¼åŒ–è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆå·²å¼ƒç”¨ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰"""
        return self._format_memory_context(memories)

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ä¸­é—´ä»¶ç»Ÿè®¡ä¿¡æ¯ (v2.0: åŒ…å«å¯å‘å¼æ£€ç´¢ç»Ÿè®¡)"""
        stats = {
            "enabled": self.enabled,
            "auto_inject": self.auto_inject,
            "auto_record": self.auto_record,
            "memory_stats": self.memory.get_stats(),
            "heuristic_enabled": self.heuristic_retriever is not None
        }

        # v2.0: æ·»åŠ å¯å‘å¼æ£€ç´¢ç»Ÿè®¡
        if self.heuristic_retriever:
            try:
                stats["heuristic_stats"] = self.heuristic_retriever.get_statistics()
            except Exception as e:
                stats["heuristic_error"] = str(e)

        return stats

    def _record_skill_usage(self, request: Dict[str, Any], response: Dict[str, Any]):
        """è®°å½•æŠ€èƒ½ä½¿ç”¨æƒ…å†µï¼ˆğŸ†• æ–°å¢ï¼‰"""
        try:
            response_text = response.get("response", "")
            message = request.get("message", "")
            provider = request.get("provider", "unknown")

            # æ£€æµ‹å“åº”ä¸­æ˜¯å¦æåˆ°äº†æŠ€èƒ½ï¼ˆé€šè¿‡ /skill-name æ¨¡å¼ï¼‰
            import re
            skill_mentions = re.findall(r'/([a-z0-9\-]+)', response_text)

            if skill_mentions:
                keywords = " ".join(self._extract_keywords(message))

                for skill_name in skill_mentions:
                    # è®°å½•ä½¿ç”¨
                    self.skills_discovery.record_usage(
                        skill_name=skill_name,
                        task_keywords=keywords,
                        provider=provider,
                        success=True
                    )

                print(f"[MemoryMiddleware] Recorded skill usage: {skill_mentions}")

        except Exception as e:
            print(f"[MemoryMiddleware] Skill usage recording error: {e}")

    def _track_injection(
        self,
        request_id: str,
        provider: str,
        original_message: str,
        memories: List[Dict[str, Any]],
        skills: Optional[Dict[str, Any]],
        system_context_injected: bool
    ):
        """è¿½è¸ªè®°å¿†æ³¨å…¥è¯¦æƒ…ï¼ˆPhase 1: Transparencyï¼‰"""
        try:
            # æå–è®°å¿† IDs å’Œç›¸å…³æ€§åˆ†æ•°
            memory_ids = []
            relevance_scores = {}
            for mem in memories:
                mem_id = mem.get("id") or mem.get("message_id")
                if mem_id:
                    memory_ids.append(mem_id)
                    # å¦‚æœæœ‰ç›¸å…³æ€§åˆ†æ•°
                    if mem.get("relevance_score"):
                        relevance_scores[mem_id] = mem.get("relevance_score")

            # æå–æŠ€èƒ½åç§°
            skill_names = []
            if skills and skills.get("found"):
                for skill in skills.get("skills", []):
                    skill_names.append(skill.get("name"))

            # ä½¿ç”¨ memory v2 è¿½è¸ª
            self.memory.v2.track_request_injection(
                request_id=request_id,
                provider=provider,
                original_message=original_message,
                injected_memory_ids=memory_ids,
                injected_skills=skill_names,
                injected_system_context=system_context_injected,
                relevance_scores=relevance_scores,
                metadata={
                    "memory_count": len(memories),
                    "skills_count": len(skill_names),
                    "system_context": system_context_injected
                }
            )

            print(f"[MemoryMiddleware] Tracked injection for {request_id}: "
                  f"{len(memory_ids)} memories, {len(skill_names)} skills")

        except Exception as e:
            print(f"[MemoryMiddleware] Injection tracking error: {e}")

    # ========================================================================
    # Discussion Memory (Phase 6)
    # ========================================================================

    async def post_discussion(
        self,
        session_id: str,
        topic: str,
        providers: List[str],
        summary: str = None,
        insights: List[Dict[str, Any]] = None,
        messages: List[Dict[str, Any]] = None
    ) -> Optional[str]:
        """Record a discussion to memory system (Phase 6)

        Args:
            session_id: Discussion session ID
            topic: Discussion topic
            providers: List of participating providers
            summary: Discussion summary
            insights: Extracted insights
            messages: Discussion messages

        Returns:
            observation_id if recorded, None otherwise
        """
        if not self.enabled or not self.auto_record:
            return None

        try:
            observation_id = self.memory.v2.record_discussion(
                session_id=session_id,
                topic=topic,
                providers=providers,
                summary=summary,
                insights=insights,
                messages=messages
            )

            print(f"[MemoryMiddleware] Discussion recorded: {session_id} -> {observation_id}")
            return observation_id

        except Exception as e:
            print(f"[MemoryMiddleware] Discussion recording error: {e}")
            return None
